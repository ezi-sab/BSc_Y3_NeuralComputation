{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Computation Exercise 4: Back Propagation\n",
    "In this exercise, we will solve a binary classification problem by a 2-layer neural network. The data is not linearly separable and therefore the perceptron algorithm introduced in the last week will not work. We will train the neural network by gradient descent. To this aim, we need to compute gradients by the backpropagation algorithm.\n",
    "\n",
    "After finishing this exercise, you will learn the following\n",
    "* forward propagation\n",
    "* backward propagation\n",
    "\n",
    "First, we will import necessary libraries. Note we can import the `sigmoid` function from the `scipy` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit as sigmoid\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will define a function `make_dataset(num_points)` to generate a two-dimensional dataset. We use `np.random.seed(0)` to initialize the random number generator. Then, different runs of the code will produce the same results.\n",
    "\n",
    "The function `make_dataset(num_points)` generates `num_points` examples. The **first half** are positive examples and the **second half** are negative examples. Note that the positive examples are of smaller distance to the zero point, while the negative examples are of larger distance to the zero point. It is clear that this dataset is not linearly separable, i.e., we cannot find a linear function to separate positive examples from negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def make_dataset(num_points):\n",
    "    radius = 5\n",
    "    data = []\n",
    "    labels = []\n",
    "    # Generate positive examples (labeled 1).\n",
    "    for i in range(num_points // 2):\n",
    "        # the radius of positive examples\n",
    "        r = np.random.uniform(0, radius*0.5)\n",
    "        angle = np.random.uniform(0, 2*math.pi)\n",
    "        x = r * math.sin(angle)\n",
    "        y = r * math.cos(angle)\n",
    "        data.append([x, y])\n",
    "        labels.append(1)\n",
    "        \n",
    "    # Generate negative examples (labeled 0).\n",
    "    for i in range(num_points // 2):\n",
    "        # the radius of negative examples\n",
    "        r = np.random.uniform(radius*0.7, radius)\n",
    "        angle = np.random.uniform(0, 2*math.pi)\n",
    "        x = r * math.sin(angle)\n",
    "        y = r * math.cos(angle)\n",
    "        data.append([x, y])\n",
    "        labels.append(0)\n",
    "        \n",
    "    data = np.asarray(data)\n",
    "    labels = np.asarray(labels)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Data\n",
    "\n",
    "We first apply `make_dataset` to generate a dataset of 200 examples. We visualise the dataset using a `scatter` plot using the scatter function in the `matplotlib.pylot` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 200\n",
    "data, labels = make_dataset(num_data)\n",
    "\n",
    "# Note: red indicates a label of 1, blue indicates a label of 0\n",
    "plt.scatter(data[:num_data//2, 0], data[:num_data//2, 1], color='red') \n",
    "plt.scatter(data[num_data//2:, 0], data[num_data//2:, 1], color='blue') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network definition\n",
    "\n",
    "We will try to classify this data by training a neural network. As a reminder, our goal is to take as input a two dimensional vector $\\mathbf{x} = [x_1, x_2]^\\top$ and output a real number from which we can predict the output label. We consider the following structure. The neural network has three layers: the first layer has $2$ nodes, the second layer has $3$ nodes and the last layer has $1$ node. In this case, the parameters we want to learn are **weight matrices**: $W^2\\in \\mathbb{R}^{3\\times 2}, W^3\\in\\mathbb{R}^{1\\times 3}$ and **bias vectors**: $b^2\\in\\mathbb{R}^3$ and $b^3\\in\\mathbb{R}$. Here we initialize the weights using the [**randn**](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html) function, and the bias as the zero vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, initialize our neural network parameters.\n",
    "params = {}\n",
    "params['W2'] = np.random.randn(3, 2)\n",
    "params['b2'] = np.zeros(3)\n",
    "params['W3'] = np.random.randn(3)\n",
    "params['b3'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "We now implement **forward propagation** to compute the output of the network. According to the `forward propagation` algorithm, we know\n",
    "$$\\mathbf{z}^2 = \\mathbf{W}^2x+\\mathbf{b}^2$$\n",
    "$$\\mathbf{a}^2 = \\sigma(\\mathbf{z}^2)$$\n",
    "$$\\mathbf{z}^3 = \\mathbf{W}^3\\mathbf{a}^2+\\mathbf{b}^3$$\n",
    "$$\\mathbf{a}^3 = \\sigma(\\mathbf{z}^3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, $\\mathbf{z}$ are linear functions of inputs and $\\mathbf{a}$ are activations of $\\mathbf{z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: complete the implementation the forward propagation by computing z2,a2,z3,a3. \n",
    "# This function returns the predicted output of the network, i.e., a3 in the above equation\n",
    "def forward(x, params):    \n",
    "    '''\n",
    "    Input:\n",
    "        x: the feature vector of an example\n",
    "        params: a dictionary containing both weight matrices and bias vectors of a neural network\n",
    "    Output:\n",
    "        return the function valued computed by the neural network\n",
    "    '''\n",
    "    z2 = \n",
    "    a2 = \n",
    "    z3 = \n",
    "    a3 = \n",
    "    return a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the network's predictions\n",
    "\n",
    "Let's visualize the predictions of our untrained network. As we can see, the network does not succeed at classifying the points without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 200\n",
    "\n",
    "# we first traverse two features in the region (-6,6)\n",
    "x1s = np.linspace(-6.0, 6.0, num_points)\n",
    "x2s = np.linspace(-6.0, 6.0, num_points)\n",
    "\n",
    "# we now use forward propagation to compute the matrix Y\n",
    "# Y[i, j] should be the output of the network on the example (x1s[i], x2s[j]) \n",
    "# To do: insert your code here the compute Y\n",
    "Y = np.zeros([num_points, num_points])\n",
    "for i in range(num_points):\n",
    "    for j in range(num_points):\n",
    "        x = np.array([x1s[i], x2s[j]])\n",
    "        Y\n",
    "# we now visualize the data\n",
    "X1, X2 = np.meshgrid(x1s, x2s)\n",
    "plt.pcolormesh(X1, X2, Y, cmap=plt.cm.get_cmap('YlGn'))\n",
    "plt.colorbar()\n",
    "plt.scatter(data[:num_data//2, 0], data[:num_data//2, 1], color='red') \n",
    "plt.scatter(data[num_data//2:, 0], data[num_data//2:, 1], color='blue') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of Sigmoid function\n",
    "\n",
    "Before implementing the backward propagation, we need to know the derivative of the sigmoid function\n",
    "\n",
    "$$\\sigma(x)=1/(1+\\exp(-x))$$\n",
    "In the following, you are requested to complete the code on computing the gradient of the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: insert your code here to compute the gradient of the sigmod function\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (slope/derivative) of the sigmoid function with\n",
    "    respect to its input x.\n",
    "\n",
    "    Args:\n",
    "     x: scalar or numpy array\n",
    "\n",
    "    Returns:\n",
    "     gradient: gradient of the sigmoid function with respect to x\n",
    "    \"\"\"\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement back propagation\n",
    "\n",
    "We now implement the back propagation to compute the gradients of the loss function on the parameters $W^2, W^3, b^2, b^3$. We consider the square loss\n",
    "$$\n",
    "C(a^3,y)=\\frac{1}{2}(a^3-y)^2,\n",
    "$$\n",
    "where $a^3$ is the output of the network with the input $\\mathbf{x}$. According to the back propagation algorithm, we need to compute the back-propagated gradients\n",
    "$$\n",
    "\\delta^3:=\\frac{\\partial C}{\\partial \\mathbf{z}^3}\\quad\\text{and}\\quad\\delta^2=\\frac{\\partial C}{\\partial \\mathbf{z}^2}.\n",
    "$$\n",
    "In particular, the `back-propagated gradients` for the **output layer** can be derived by (note that $\\delta^3\\in\\mathbb{R}$ since the last layer only has 1 node)\n",
    "$$\n",
    "\\delta^3=\\sigma'(z^3)(a^3-y).\n",
    "$$\n",
    "Then, the recursive relationship allows us to compute the `back-propagated gradients` for the **hidden layer** as follows\n",
    "$$\n",
    "\\delta^2=(\\mathbf{W}^3)^\\top\\delta^3\\odot \\sigma'(\\mathbf{z}^2).\n",
    "$$\n",
    "This finishes the computation of the `back-propagated gradients`. We now can use these `back-propagated gradients` to compute the gradients of the loss $C$ on $\\mathbf{W}^2, \\mathbf{W}^3, \\mathbf{b}^2, \\mathbf{b}^3$. According to our discussion in the lecture, we know\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial \\mathbf{W}^{\\ell}}=\\delta^\\ell(\\mathbf{a}^{\\ell-1})^\\top,\\qquad\\ell=2,3.\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial \\mathbf{b}^\\ell}=\\delta^\\ell\\qquad\\ell=2,3.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(x, y, params):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with parameters W2, W3, b2 and b3\n",
    "\n",
    "    Args:\n",
    "     x: the input feature, a d dimensional vector\n",
    "     y: the output, a real number\n",
    "     params: the parameters W2, W3, b2 and b3\n",
    "\n",
    "    Returns:\n",
    "     grads: the gradient of the loss with parameters W2, W3, b2 and b3\n",
    "     loss: the loss incurred by using the neural network on the example\n",
    "    \"\"\"\n",
    "    # Perform forwards computation.\n",
    "    W2 = params['W2']\n",
    "    W3 = params['W3']\n",
    "    \n",
    "    # To Do: insert your code to compute z2, a2, z3, a3 and loss\n",
    "    z2 = \n",
    "    a2 = \n",
    "    z3 = \n",
    "    a3 = \n",
    "    loss = \n",
    "    \n",
    "    # Perform backwards computation.\n",
    "    # To Do: insert your code to compute z3_bar, z2_bar, W3_bar, W2_bar, b3_bar, b2_bar\n",
    "    '''\n",
    "    z3_bar is the gradient of C w.r.t. z3\n",
    "    z2_bar is the gradient of C w.r.t. z2\n",
    "    w3_bar is the gradient of C w.r.t. w3\n",
    "    w2_bar is the gradient of C w.r.t. w2  \n",
    "    b3_bar is the gradient of C w.r.t. b3\n",
    "    b2_bar is the gradient of C w.r.t. b2\n",
    "    Hint: you may need the np.outer function to realize the multiplication v1 * v2.T for two vectors v1, v2\n",
    "    '''\n",
    "    z3_bar = \n",
    "    z2_bar = \n",
    "    W3_bar = \n",
    "    W2_bar = \n",
    "    b3_bar = \n",
    "    b2_bar = \n",
    "    \n",
    "    # Wrap our gradients in a dictionary.\n",
    "    grads = {}\n",
    "    grads['W3'] = W3_bar\n",
    "    grads['W2'] = W2_bar\n",
    "    grads['b3'] = b3_bar\n",
    "    grads['b2'] = b2_bar\n",
    "    \n",
    "    return grads, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network\n",
    "\n",
    "We can train our network parameters using gradient descent once we have computed derivatives using the backpropagation algorithm. Recall that the gradient descent update rule for a given parameter $p$ and a learning rate $\\eta$ is:\n",
    "\n",
    "$$p \\gets p - \\eta * \\frac{\\partial C}{\\partial p}.$$\n",
    "Note $C$ should be an average of the error on all training examples for applying gradient descent. Our backpropagation algorithm only computes the gradient based on a **single** example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# First, initialize our neural network parameters.\n",
    "params = {}\n",
    "params['W2'] = np.random.randn(3, 2)\n",
    "params['b2'] = np.zeros(3)\n",
    "params['W3'] = np.random.randn(3)\n",
    "params['b3'] = 0\n",
    "\n",
    "num_steps = 1000\n",
    "eta = 5\n",
    "for step in range(num_steps):  \n",
    "    grads = {}\n",
    "    grads['W2'] = np.zeros((3, 2))\n",
    "    grads['b2'] = np.zeros(3)\n",
    "    grads['W3'] = np.zeros(3)\n",
    "    grads['b3'] = 0\n",
    "    loss = 0\n",
    "    for i in range(num_data): \n",
    "        \"\"\"\n",
    "        tgrads is the gradient of the network w.r.t. the parameter in the i-th training example\n",
    "        tloss is the loss in the i-th training example\n",
    "        we first sum the gradients on all training examples, which is then divided by num_data to get an average\n",
    "        \"\"\"\n",
    "        tgrads, tloss = backprop(data[i, :], labels[i], params)\n",
    "        loss += tloss        \n",
    "        for k in grads:\n",
    "            grads[k] += tgrads[k]\n",
    "    # we now compute an average of loss\n",
    "    loss /= num_data  \n",
    "    for k in params:\n",
    "        # To do: insert your code to do gradient descent\n",
    "        grads[k] = \n",
    "        params[k] = \n",
    "        \n",
    "        \n",
    "\n",
    "    # Print loss every 25 iterations\n",
    "    if step % 25 == 0:\n",
    "        print(\"Step {:3d} | Loss {:3.2f}\".format(step, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the predictions\n",
    "\n",
    "We now visualize the results of the network trained by gradient descent. We expect this trained network would have much better prediction performance. Note the network outputs a value in $[0,1]$. Therefore, a natural choice for prediction is to predict a **positive example** if the output is larger than $0.5$, and **a negative example** otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 200\n",
    "x1s = np.linspace(-6.0, 6.0, num_points)\n",
    "x2s = np.linspace(-6.0, 6.0, num_points)\n",
    "Y = np.zeros([num_points, num_points])\n",
    "\n",
    "# Y[i, j] should be the output of the network on the example (x1s[i], x2s[j]) \n",
    "# To do: insert your code here the compute Y\n",
    "for i in range(num_points):\n",
    "    for j in range(num_points):\n",
    "        x = np.array([x1s[i], x2s[j]])\n",
    "        Y[i, j] = \n",
    "X1, X2 = np.meshgrid(x1s, x2s)\n",
    "plt.pcolormesh(X1, X2, Y, cmap=plt.cm.get_cmap('YlGn'))\n",
    "plt.colorbar()\n",
    "plt.scatter(data[:num_data//2, 0], data[:num_data//2, 1], color='red') \n",
    "plt.scatter(data[num_data//2:, 0], data[num_data//2:, 1], color='blue') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated above, the output on most positive examples have values larger than 0.5, and the output on most negative examples have values smaller than 0.5. This means that the trained network indeed does a good performance on classification in this problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute its accuracy on the training dataset. To this aim, we go through all training examples and compute the predicted label. Since the sigmoid function outputs the number between 0 and 1, we can use 0.5 as the threshold. That is, we predict it as a positiv example if the output of the network is larger than 0.5, and a negative example otherwise. As you can see, we achieve a very high accuracy. This demonstrates the efficiency of the network in solving the binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predicted_y = np.zeros(num_data)\n",
    "for i in range(num_data): \n",
    "    predicted_y[i] = forward(data[i, :], params)\n",
    "    if predicted_y[i]>0.5:\n",
    "        predicted_y[i] = 1\n",
    "    else:\n",
    "        predicted_y[i] = 0\n",
    "acc_tr = accuracy_score(predicted_y, labels)\n",
    "print(acc_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch Gradient Descent\n",
    "\n",
    "In the above implementation, we use gradient descent to train the neural network. We now use minibatch gradient descent to train the neural network. The basic idea is to use a subsample to compute an approximate gradient. \n",
    "\n",
    "The optimization problem in ML often has a **sum** structure in the sense\n",
    "$$\n",
    "C(\\mathbf{w})=\\frac{1}{n}\\sum_{i=1}^nC_i(\\mathbf{w}),\n",
    "$$\n",
    "where $C_i(\\mathbf{w})$ is the loss of the model $\\mathbf{w}$ on the $i$-th example. In our problem, $C_i$ takes the form $C_i(\\mathbf{w})=\\frac{1}{2}(nn(x^{(i)})-y^{(i)})^2$, where $nn(x^{(i)})$ is the output of the neural network with the input $x^{(i)}$.\n",
    "\n",
    "Gradient descent requires to go through all training examples to compute a single gradient, which may be time consuming if the sample size is large. Minibatch gradient descent improves the efficiency by using a subset of training examples to build an **approximate** gradient. At each iteration, it first randomly draws a set $I\\subset\\{1,2,\\ldots,n\\}$ of size $s$, where we often call $s$ the minibatch size. Then it builds an approximate gradient by\n",
    "$$\n",
    "\\nabla_I(\\mathbf{w}^{(t)})=\\frac{1}{s}\\sum_{i\\in I}\\nabla C_i(\\mathbf{w}^{(t)})\n",
    "$$\n",
    "Now, it updates the model by\n",
    "$$\n",
    "\\mathbf{w}^{(t+1)}=\\mathbf{w}^{(t)}-\\eta_t\\nabla_I(\\mathbf{w}^{(t)}).\n",
    "$$ \n",
    "It is recommended to use $s\\in[20,100]$. Depending on different $s$, minibatch gradient descent recovers several algorithms\n",
    "\\begin{align*}\n",
    "  s<n \\;&\\Rightarrow\\;\\text{Minibatch gradient descent}\\\\\n",
    "  s=1 \\;&\\Rightarrow\\;\\text{Stochastic gradient descent} \\\\\n",
    "  s=n \\;&\\Rightarrow\\;\\text{Batch gradient descent}\n",
    "\\end{align*}\n",
    "In the following, we request you to finish the following implementation of the `minibatch gradient descent` on the linear regression problem. To search a subset of $\\{1,2,\\ldots,n\\}$, we recommend you to use the function `random.sample`. The synatx is `random.sample(sequence, k)`, which returns $k$ length new list of elements chosen from the `sequence`. More details can be found  [here](https://www.geeksforgeeks.org/python-random-sample-function/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "np.random.seed(0)\n",
    "\n",
    "# First, initialize our neural network parameters.\n",
    "params = {}\n",
    "params['W2'] = np.random.randn(3, 2)\n",
    "params['b2'] = np.zeros(3)\n",
    "params['W3'] = np.random.randn(3)\n",
    "params['b3'] = 0\n",
    "\n",
    "num_steps = 500\n",
    "eta = 5\n",
    "\n",
    "batch_size = 25\n",
    "tset = list(range(num_data))\n",
    "for step in range(num_steps):  \n",
    "    grads = {}\n",
    "    grads['W2'] = np.zeros((3, 2))\n",
    "    grads['b2'] = np.zeros(3)\n",
    "    grads['W3'] = np.zeros(3)\n",
    "    grads['b3'] = 0\n",
    "    loss = 0\n",
    "    # To do, use the random.sample function to select a subset of examples to compute the gradient\n",
    "    idx = \n",
    "    for i in idx: \n",
    "        \"\"\"\n",
    "        tgrads is the gradient of the network w.r.t. the parameter in the i-th training example\n",
    "        tloss is the loss in the i-th training example\n",
    "        we first sum the gradients on all training examples, which is then divided by num_data to get an average\n",
    "        \"\"\"\n",
    "        tgrads, tloss = backprop(data[i, :], labels[i], params)\n",
    "        loss += tloss\n",
    "        for k in grads:\n",
    "            grads[k] += tgrads[k]\n",
    "    # we now compute an average of loss\n",
    "    loss /= batch_size  \n",
    "    for k in params:\n",
    "        # To do: insert your code to do gradient descent\n",
    "        grads[k] =\n",
    "        params[k] ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can see the performance of the trained network with the minibatch gradient descent. It still has a good performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 200\n",
    "predicted_y = np.zeros(num_data)\n",
    "for i in range(num_data): \n",
    "    predicted_y[i] = forward(data[i, :], params)\n",
    "    if predicted_y[i]>0.5:\n",
    "        predicted_y[i] = 1\n",
    "    else:\n",
    "        predicted_y[i] = 0\n",
    "acc_tr = accuracy_score(predicted_y, labels)\n",
    "print(acc_tr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
