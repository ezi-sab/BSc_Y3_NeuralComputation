{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Computation Exercise 5: Advanced Optimization Algorithms\n",
    "\n",
    "In this exercise, we'll develop implementations of advanced optimization algorithms. As in Exercise 2, we will use the Boston Housing dataset and run some advanced optimization algorithms to solved the linear regression problems.\n",
    "\n",
    "In this exercise, you will learn the following\n",
    "* implement the `momentum` method\n",
    "* implement the `Nesterov momentum` method\n",
    "* implement the `minibatch gradient descent` method\n",
    "* implement the `adaptive (stochastic) gradient descent` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import preprocessing   # for normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston Housing Data\n",
    "\n",
    "The Boston Housing data is one of the  datasets available in sklearn.\n",
    "We can import the dataset and preprocess it as follows. Note we add a feature of $1$ to `x_input` to get a n x (d+1) matrix `x_in`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    boston_data = load_boston()\n",
    "data = boston_data['data']\n",
    "x_input = data  # a data matrix\n",
    "y_target = boston_data['target'] # a vector for all outputs\n",
    "# add a feature 1 to the dataset, then we do not need to consider the bias and weight separately\n",
    "x_in = np.concatenate([np.ones([np.shape(x_input)[0], 1]), x_input], axis=1)\n",
    "# we normalize the data so that each has regularity\n",
    "x_in = preprocessing.normalize(x_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model\n",
    "\n",
    "A linear regression model in one variable has the following form \n",
    "$$\n",
    "f(x)=\\mathbf{w}^\\top \\mathbf{x}.\n",
    "$$\n",
    "The following function computes the output of the linear model on a data matrix of size n x (d+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearmat_2(w, X):\n",
    "    '''\n",
    "    a vectorization of linearmat_1 in lab 2.\n",
    "    Input: w is a weight parameter (including the bias), and X is a data matrix (n x (d+1)) (including the feature)\n",
    "    Output: a vector containing the predictions of linear models\n",
    "    '''\n",
    "    return np.dot(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "We defined the following `mean square error` function for a linear regression problem using the square loss:\n",
    "$$\n",
    "C(\\mathbf{y}, \\mathbf{t}) = \\frac{1}{2n}(\\mathbf{y}-\\mathbf{t})^\\top (\\mathbf{y}-\\mathbf{t}).\n",
    "$$\n",
    "The python implementation is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(w, X, y):\n",
    "    '''\n",
    "    Evaluate the cost function in a vectorized manner for \n",
    "    inputs `X` and outputs `y`, at weights `w`.\n",
    "    '''\n",
    "    residual = y - linearmat_2(w, X)  # get the residual\n",
    "    err = np.dot(residual, residual) / (2 * len(y)) # compute the error\n",
    "    \n",
    "    return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Computation\n",
    "\n",
    "Our methods require to use the gradient of the `cost` function. As discussed in the previous lecture, the gradient can be computed by\n",
    "$$\\nabla C(\\mathbf{w}) =\\frac{1}{n}X^\\top\\big(X\\mathbf{w}-\\mathbf{y}\\big)$$\n",
    "In the following, we present the python implementation on the gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized gradient function\n",
    "def gradfn(weights, X, y):\n",
    "    '''\n",
    "    Given `weights` - a current \"Guess\" of what our weights should be\n",
    "          `X` - matrix of shape (N,d+1) of input features including the feature $1$\n",
    "          `y` - target y values\n",
    "    Return gradient of each weight evaluated at the current value\n",
    "    '''\n",
    "\n",
    "    y_pred = np.dot(X, weights)\n",
    "    error = y_pred - y\n",
    "    return np.dot(X.T, error) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient Descent iteratively updates the model by moving along the negative direction\n",
    "$$\\mathbf{w}^{(t+1)} \\leftarrow \\mathbf{w}^{(t)} - \\eta\\nabla C(\\mathbf{w}^{(t)}),$$ \n",
    "where $\\eta$ is a learning rate and $\\nabla C(w^{(t)})$ is the gradient evaluated at current parameter value $\\mathbf{w}^{(t)}$. In the following, we give the python implementation of the gradient descent on the linear regression problem. Here, we use `idx_res` to store the indices of iterations where we have computed the cost, and use `err_res` to store the cost of models at these iterations. These will be used to plot how the `cost` will behave `versus iteration` number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_via_gradient_descent(X, y, print_every=100,\n",
    "                               niter=2000, eta=1):\n",
    "    '''\n",
    "    Solves for linear regression weights with gradient descent.\n",
    "    Given `X` - matrix of shape (N,D) of input features\n",
    "          `y` - target y values\n",
    "          `print_every` - we report performance every 'print_every' iterations\n",
    "          `niter` - the number of iterates allowed\n",
    "          `eta` - learning rate\n",
    "    \n",
    "    Return \n",
    "        `w` - weights after `niter` iterations\n",
    "        `idx_res` - the indices of iterations where we compute the cost\n",
    "        `err_res` - the cost at iterations indicated by idx_res\n",
    "    '''\n",
    "    N, D = np.shape(X)\n",
    "    # initialize all the weights to zeros\n",
    "    w = np.zeros([D])\n",
    "    idx_res = []\n",
    "    err_res = []\n",
    "    for k in range(niter):\n",
    "        # compute the gradient\n",
    "        dw = gradfn(w, X, y)\n",
    "        # gradient descent\n",
    "        w = w - eta * dw\n",
    "        # we report the progress every print_every iterations\n",
    "        # note the operator % calculates the remainder of dividing two values\n",
    "        if k % print_every == print_every - 1:\n",
    "            t_cost = cost(w, X, y)\n",
    "            print('error after %d iteration: %s' % (k, t_cost))\n",
    "            idx_res.append(k)\n",
    "            err_res.append(t_cost)\n",
    "    return w, idx_res, err_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply **gradient descent** to solve the **Boston House Price** prediction problem, and get the weight `w_gd`, the indices `idx_gd` and the errors 'err_gd' on these indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_gd, idx_gd, err_gd = solve_via_gradient_descent( X=x_in, y=y_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "Momentum introduces a variable `velocity` to store the historical information of the gradients. At each iteration, it updates `velocity` as a factor of the current `velocity` minus the `learning rate` times the `current gradient`\n",
    "$$\\mathbf{v}^{(t+1)} = \\alpha\\mathbf{v}^{(t)}-\\eta\\nabla C(\\mathbf{w}^{(t)}),$$ \n",
    "where $\\eta$ is a learning rate, $\\alpha\\in(0,1)$ is a parameter and $\\nabla C(w^{(t)}$ is the gradient evaluated at current parameter value $\\mathbf{w}^{(t)}$.\n",
    "Then, we update the next iterate as \n",
    "$$\\mathbf{w}^{(t+1)}=\\mathbf{w}^{(t)}+\\mathbf{v}^{(t+1)}.$$\n",
    "In the following, we request you to finish the following implementation of the `momentum` on the linear regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_via_momentum(X, y, print_every=100,\n",
    "                               niter=2000, eta=1, alpha=0.8):\n",
    "    '''\n",
    "    Solves for linear regression weights with momentum.\n",
    "    Given `X` - matrix of shape (N,D) of input features\n",
    "          `y` - target y values\n",
    "          `print_every` - we report performance every 'print_every' iterations\n",
    "          `niter` - the number of iterates allowed\n",
    "          `eta` - learning rate\n",
    "          `alpha` - determines the influence of past gradients on the current update\n",
    "\n",
    "    Return \n",
    "        `w` - weights after `niter` iterations\n",
    "        `idx_res` - the indices of iterations where we compute the cost\n",
    "        `err_res` - the cost at iterations\n",
    "    '''\n",
    "    N, D = np.shape(X)\n",
    "    # initialize all the weights to zeros\n",
    "    w = np.zeros([D])\n",
    "    v = np.zeros([D])\n",
    "    idx_res = []\n",
    "    err_res = []\n",
    "    for k in range(niter):\n",
    "        # TODO: Insert your code to update w by momentum\n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        if k % print_every == print_every - 1:\n",
    "            t_cost = cost(w, X, y)\n",
    "            print('error after %d iteration: %s' % (k, t_cost))\n",
    "            idx_res.append(k)\n",
    "            err_res.append(t_cost)\n",
    "    return w, idx_res, err_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply **momentum** to solve the **Boston House Price** prediction problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_momentum, idx_momentum, err_momentum = solve_via_momentum( X=x_in, y=y_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between Gradient Descent and Gradient Descent with Momentum\n",
    "\n",
    "We can now compare the behavie of Gradient Descent and Gradient Descent with Momentum. In particular, we will show how the `cost` of models found by the algorithm at different iterations would behave with respect to the iteration number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plot the cost w.r.t. the iteration number\n",
    "plt.plot(idx_gd, err_gd, color=\"red\", linewidth=2.5, linestyle=\"-\", label=\"gradient descent\") #  gradient descent\n",
    "plt.plot(idx_momentum, err_momentum, color=\"blue\", linewidth=2.5, linestyle=\"-\", label=\"momentum\") # momentum\n",
    "plt.legend(loc='upper right', prop={'size': 12})\n",
    "plt.title('comparison between gradient descent and momentum')\n",
    "plt.xlabel(\"number of iterations\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, **gradient descent with momentum** is much faster than the **gradient descent**. This shows the benefit of using velocity to store historical gradient information for accelerating the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Momentum\n",
    "\n",
    "Anoter algorithm which can acclerate the training speed of gradient descent is the **Nesterov Momentum**. Analogous to **Momentum**, Nesterov Momentum also introduces a variable `velocity` to store the historical information of the gradients. The difference is that it first uses the current velocity to build a `looking ahead` point. Then the gradient computation is performed at the  `looking ahead` point. The `looking ahead` point may contain more information than the current point. Therefore, the gradient at `looking ahead` point may be more precise than the `current gradient`.\n",
    "The update equation is as follows\n",
    "$$\\mathbf{w}^{\\text{(ahead)}}=\\mathbf{w}^{(t)}+\\alpha\\mathbf{v}^{(t)}$$\n",
    "$$\\mathbf{v}^{(t+1)} = \\alpha\\mathbf{v}^{(t)}-\\eta\\nabla C(\\mathbf{w}^{(\\text{ahead})}),$$ \n",
    "where $\\eta$ is a learning rate and $\\alpha\\in(0,1)$ is a parameter.\n",
    "Then, we update the next iterate as \n",
    "$$\\mathbf{w}^{(t+1)}=\\mathbf{w}^{(t)}+\\mathbf{v}^{(t+1)}.$$\n",
    "In the following, we request you to finish the following implementation of the `Nesterov Momentum` on the linear regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_via_nag(X, y, print_every=100,\n",
    "                               niter=2000, eta=0.5, alpha=0.8):\n",
    "    '''\n",
    "    Solves for linear regression weights with nesterov momentum.\n",
    "    Given `X` - matrix of shape (N,D) of input features\n",
    "          `y` - target y values\n",
    "          `print_every` - we report performance every 'print_every' iterations\n",
    "          `niter` - the number of iterates allowed\n",
    "          `eta` - learning rate\n",
    "          `alpha` - determines the influence of past gradients on the current update\n",
    "\n",
    "    Return \n",
    "        `w` - weights after `niter` iterations\n",
    "        `idx_res` - the indices of iterations where we compute the cost\n",
    "        `err_res` - the cost at iterations\n",
    "    '''\n",
    "    N, D = np.shape(X)\n",
    "    # initialize all the weights to zeros\n",
    "    w = np.zeros([D])\n",
    "    v = np.zeros([D])\n",
    "    idx_res = []\n",
    "    err_res = []\n",
    "    for k in range(niter):\n",
    "        # TODO: Insert your code to update w by nesterov momentum\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        if k % print_every == print_every - 1:\n",
    "            t_cost = cost(w, X, y)\n",
    "            print('error after %d iteration: %s' % (k, t_cost))\n",
    "            idx_res.append(k)\n",
    "            err_res.append(t_cost)\n",
    "    return w, idx_res, err_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply nesterov momentum to solve the Boston House Price prediction problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_nag, idx_nag, err_nag = solve_via_nag( X=x_in, y=y_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between Gradient Descent and Nesterov Momentum\n",
    "\n",
    "We can now compare the behavie of Gradient Descent and Nesterov Momentum. In particular, we will show how the `cost` of models found by the algorithm at different iterations would behave with respect to the iteration number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(idx_gd, err_gd, color=\"red\", linewidth=2.5, linestyle=\"-\", label=\"gradient descent\")\n",
    "plt.plot(idx_nag, err_nag, color=\"blue\", linewidth=2.5, linestyle=\"-\", label=\"nesterov momentum\")\n",
    "plt.legend(loc='upper right', prop={'size': 12})\n",
    "plt.title('comparison between gradient descent and momentum')\n",
    "plt.xlabel(\"number of iterations\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected, **nesterov momentum** is much faster than the **gradient descent**. This  again shows the benefit of using velocity to store historical gradient information for accelerating the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch Gradient Descent\n",
    "\n",
    "The optimization problem in ML often has a **sum** structure in the sense\n",
    "$$\n",
    "C(\\mathbf{w})=\\frac{1}{n}\\sum_{i=1}^nC_i(\\mathbf{w}),\n",
    "$$\n",
    "where $C_i(\\mathbf{w})$ is the loss of the model $\\mathbf{w}$ on the $i$-th example. In our Boston House Price prediction problem, $C_i$ takes the form $C_i(\\mathbf{w})=\\frac{1}{2}(\\mathbf{w}^\\top\\mathbf{x}^{(i)}-y^{(i)})^2$.\n",
    "\n",
    "Gradient descent requires to go through all training examples to compute a single gradient, which may be time consuming if the sample size is large. Minibatch gradient descent improves the efficiency by using a subset of training examples to build an **approximate** gradient. At each iteration, it first randomly draws a set $I\\subset\\{1,2,\\ldots,n\\}$ of size $s$, where we often call $s$ the minibatch size. Then it builds an approximate gradient by\n",
    "$$\n",
    "\\nabla^I(\\mathbf{w}^{(t)})=\\frac{1}{s}\\sum_{i\\in I}\\nabla C_i(\\mathbf{w}^{(t)})\n",
    "$$\n",
    "Now, it updates the model by\n",
    "$$\n",
    "\\mathbf{w}^{(t+1)}=\\mathbf{w}^{(t)}-\\eta_t\\nabla^I(\\mathbf{w}^{(t)}).\n",
    "$$ \n",
    "It is recommended to use $s\\in[20,100]$. Depending on different $s$, minibatch gradient descent recovers several algorithms\n",
    "\\begin{align*}\n",
    "  s<n \\;&\\Rightarrow\\;\\text{Minibatch gradient descent}\\\\\n",
    "  s=1 \\;&\\Rightarrow\\;\\text{Stochastic gradient descent} \\\\\n",
    "  s=n \\;&\\Rightarrow\\;\\text{Batch gradient descent}\n",
    "\\end{align*}\n",
    "In the following, we request you to finish the following implementation of the `minibatch gradient descent` on the linear regression problem. To search a subset of $\\{1,2,\\ldots,n\\}$, we recommend you to use the function `random.sample`. The synatx is `random.sample(sequence, k)`, which returns $k$ length new list of elements chosen from the `sequence`. More details can be found  [here](https://www.geeksforgeeks.org/python-random-sample-function/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_via_minibatch(X, y, print_every=100,\n",
    "                               niter=5000, eta=0.2, batch_size=50):\n",
    "    '''\n",
    "    Solves for linear regression weights with nesterov momentum.\n",
    "    Given `X` - matrix of shape (N,D) of input features\n",
    "          `y` - target y values\n",
    "          `print_every` - we report performance every 'print_every' iterations\n",
    "          `niter` - the number of iterates allowed\n",
    "          `eta` - learning rate\n",
    "          `batch_size` - the size of minibatch\n",
    "    Return \n",
    "        `w` - weights after `niter` iterations\n",
    "        `idx_res` - the indices of iterations where we compute the cost\n",
    "        `err_res` - the cost at iterations\n",
    "    '''\n",
    "    N, D = np.shape(X)\n",
    "    # initialize all the weights to zeros\n",
    "    w = np.zeros([D])\n",
    "    idx_res = []\n",
    "    err_res = []\n",
    "    tset = list(range(N))\n",
    "    for k in range(niter):\n",
    "        # TODO: Insert your code to update w by minibatch gradient descent\n",
    "        idx = random.sample(tset, batch_size)\n",
    "        #sample batch of data\n",
    "        sample_X = X[idx, :]\n",
    "        sample_y = y[idx]\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        if k % print_every == print_every - 1:\n",
    "            t_cost = cost(w, X, y)\n",
    "            print('error after %d iteration: %s' % (k, t_cost))\n",
    "            idx_res.append(k)\n",
    "            err_res.append(t_cost)\n",
    "    return w, idx_res, err_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply minibatch gradient descent to solve the Boston House Price prediction problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_batch, idx_batch, err_batch = solve_via_minibatch( X=x_in, y=y_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Gradient Descent\n",
    "\n",
    "Stochastic gradient descent uses the same learning rates for all the features. This can be problematic if there are some sparse and predictive features. The underlying reason is that there are few examples with non-zero values for the sparse features, and it is often the case that SGD will choose an example with a zero for the sparse feature. Then it would not update the corresponding coordinate in this case. This motivates the need to slow down the update of some coordinates if there is already a frequent update on that coordinate, and accelerate the update if there are few updates on that coordinate.\n",
    "\n",
    "The key idea of `Adagrad` is to introduce a vector $\\mathbf{r}$ to store the accumulated gradient norm square. We initialize $\\mathbf{r}^{(0)}=0$ and update\n",
    "$$ \n",
    "\\mathbf{r}^{(t+1)}=\\mathbf{r}^{(t)}+\\hat{\\mathbf{g}}^{(t)}\\odot\\hat{\\mathbf{g}}^{(t)},\n",
    "$$\n",
    "where $\\hat{\\mathbf{g}}^{(t)}$ can be a stochastic gradient built based on a selected example or a minibatch of examples. Note $\\mathbf{r}^{(t+1)}$ records the accumulated magnitude square of gradients in each coordinate up to the $t$-th iteration. In this way, the entries of $\\mathbf{r}^{(t+1)}$ would be different. If ${r}_j^{(t+1)}$ is large, then this means that there are a lot of updates on the $j$-th coordinate. If ${r}_j^{(t+1)}$ is small, then this means that there are few updates on the $j$-th coordinate. The idea is to slow down the update on a coordinate if there are already many updates on that coordinate in the history, and speed up the update on a coordinate if there are few updates in the history. This can be achieved by dividing the parameter $\\eta$ with $\\sqrt{\\mathbf{r}^{(t+1)}}$. That is\n",
    "$$\n",
    "\\mathbf{w}^{(t+1)}\\gets\\mathbf{w}^{(t)}-\\frac{\\eta}{\\delta+\\sqrt{\\mathbf{r}^{(t+1)}}}\\odot \\hat{\\mathbf{g}}^{(t)} \n",
    "$$\n",
    "In this way, we can have different learning rates on different coordinates. In the following, we request you to finish the following implementation of the `AdaGrad` on the linear regression problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_via_adagrad(X, y, print_every=100,\n",
    "                               niter=5000, eta=1, batch_size=50):\n",
    "    '''\n",
    "    Solves for linear regression weights with nesterov momentum.\n",
    "    Given `X` - matrix of shape (N,D) of input features\n",
    "          `y` - target y values\n",
    "          `print_every` - we report performance every 'print_every' iterations\n",
    "          `niter` - the number of iterates allowed\n",
    "          `eta` - learning rate\n",
    "          `batch_size` - the size of minibatch\n",
    "    Return \n",
    "        `w` - weights after `niter` iterations\n",
    "        `idx_res` - the indices of iterations where we compute the cost\n",
    "        `err_res` - the cost at iterations\n",
    "    '''\n",
    "    N, D = np.shape(X)\n",
    "    # initialize all the weights to zeros\n",
    "    w = np.zeros([D])\n",
    "    idx_res = []\n",
    "    err_res = []\n",
    "    tset = list(range(N))\n",
    "    gradients_sum = np.zeros([D])\n",
    "    delta = 1e-8\n",
    "    for k in range(niter):\n",
    "        # TODO: Insert your code to update w by Adagrad\n",
    "        idx = random.sample(tset, batch_size)\n",
    "        #sample batch of data\n",
    "        sample_X = X[idx, :]\n",
    "        sample_y = y[idx]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if k % print_every == print_every - 1:\n",
    "            t_cost = cost(w, X, y)\n",
    "            print('error after %d iteration: %s' % (k, t_cost))\n",
    "            idx_res.append(k)\n",
    "            err_res.append(t_cost)\n",
    "    return w, idx_res, err_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply adaptive gradient descent to solve the Boston House Price prediction problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_adagrad, idx_adagrad, err_adagrad = solve_via_adagrad( X=x_in, y=y_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "For the `gradient descent with momentum`, we introduce a `velocity` to store the information of historical gradients to accerlate the optimization speed. For the `AdaGrad`/ `RMSProp`, we introduce an `accumulated gradient norm square` to store the information of historical updates on all coordinates, which allows us to have different learning rates at different features. \n",
    "The basic idea of Adam is to combine the idea of `gradient descent with momentum` and `AdaGrad`/ `RMSProp` together. It introduces both a `velocity` and an `accumulated gradient norm square`, both of which are initialized with the zero vector. Let $\\hat{\\mathbf{g}}^{(t)}$ be a stochastic gradient built based on either a selected example or a minibatch of examples. It first updates the velocity $\\mathbf{s}$ by\n",
    "$$\n",
    "\\mathbf{s}^{(t+1)}=\\beta_1\\mathbf{s}^{(t)}+(1-\\beta_1)\\hat{\\mathbf{g}}^{(t)}.\n",
    "$$\n",
    "Then it updates the `accumulated gradient norm square` by\n",
    "$$\n",
    "\\mathbf{r}^{(t+1)}=\\beta_2\\mathbf{r}^{(t)}+(1-\\beta_2)\\hat{\\mathbf{g}}^{(t)}\\odot\\hat{\\mathbf{g}}^{(t)}.\n",
    "$$\n",
    "After that we need to apply a bias correct to remove the bias in the above update\n",
    "$$\n",
    "\\hat{\\mathbf{s}}^{(t+1)}=\\mathbf{s}^{(t+1)}/(1-\\beta_1^{t+1}),\\quad\n",
    "\\hat{\\mathbf{r}}^{(t+1)}=\\mathbf{r}^{(t+1)}/(1-\\beta_2^{t+1}).\n",
    "$$\n",
    "We can now update the model by\n",
    "$$\n",
    "\\mathbf{w}^{(t+1)}\\gets\\mathbf{w}^{(t)}-\\frac{\\eta}{\\delta+\\sqrt{\\hat{\\mathbf{r}}^{(t+1)}}}\\odot \\hat{\\mathbf{s}}^{(t+1)}.\n",
    "$$\n",
    "As you can see, there are four parameters in Adam: $\\eta, \\delta, \\beta_1, \\beta_2$. Some recommended choices are\n",
    "$$\n",
    "\\eta=0.001,\\quad \\beta_1=0.9,\\quad\\beta_2=0.999,\\quad \\delta=10^{-8}.\n",
    "$$\n",
    "In the following, we request you to finish the following implementation of the `Adam` on the linear regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_via_adam(X, y, print_every=100,\n",
    "                               niter=5000, eta=0.1, beta1 = 0.9, beta2 = 0.999, batch_size=50):\n",
    "    '''\n",
    "    Solves for linear regression weights with nesterov momentum.\n",
    "    Given `X` - matrix of shape (N,D) of input features\n",
    "          `y` - target y values\n",
    "          `print_every` - we report performance every 'print_every' iterations\n",
    "          `niter` - the number of iterates allowed\n",
    "          `eta` - learning rate\n",
    "          `beta1` - the parameter on updating velocity\n",
    "          `beta2` - the parameter on updating the accumulated gradient norm square\n",
    "          `batch_size` - the size of minibatch\n",
    "    Return \n",
    "        `w` - weights after `niter` iterations\n",
    "        `idx_res` - the indices of iterations where we compute the cost\n",
    "        `err_res` - the cost at iterations\n",
    "    '''\n",
    "    N, D = np.shape(X)\n",
    "    # initialize all the weights to zeros\n",
    "    w = np.zeros([D])\n",
    "    v = np.zeros([D])    \n",
    "    idx_res = []\n",
    "    err_res = []\n",
    "    tset = list(range(N))\n",
    "    gsquare = np.zeros([D])\n",
    "    delta = 1e-8\n",
    "    for k in range(niter):\n",
    "        # TODO: Insert your code to update w by Adam\n",
    "        idx = random.sample(tset, batch_size)\n",
    "        #sample batch of data\n",
    "        sample_X = X[idx, :]\n",
    "        sample_y = y[idx]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if k % print_every == print_every - 1:\n",
    "            t_cost = cost(w, X, y)\n",
    "            print('error after %d iteration: %s' % (k, t_cost))\n",
    "            idx_res.append(k)\n",
    "            err_res.append(t_cost)\n",
    "    return w, idx_res, err_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply Adam to solve the Boston House Price prediction problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_adam, idx_adam, err_adam = solve_via_adam( X=x_in, y=y_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between Minibatch Gradient Descent, Adaptive Gradient Descent and Adam\n",
    "\n",
    "We can now compare the behavie of Minibatch Gradient Descent, Adaptive Gradient Descent and Adam. In particular, we will show how the `cost` of models found by the algorithm at different iterations would behave with respect to the iteration number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(idx_batch, err_batch, color=\"red\", linewidth=2.5, linestyle=\"-\", label=\"minibatch\")\n",
    "plt.plot(idx_adagrad, err_adagrad, color=\"blue\", linewidth=2.5, linestyle=\"-\", label=\"adagrad\")\n",
    "plt.plot(idx_adam, err_adam, color=\"green\", linewidth=2.5, linestyle=\"-\", label=\"adam\")\n",
    "plt.legend(loc='upper right', prop={'size': 12})\n",
    "plt.title('comparison between minibatch gradient descent and adaptive gradient descent')\n",
    "plt.xlabel(\"number of iterations\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.grid()\n",
    "plt.show()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, Adam achieves the best performance. This demonstrates the effectiveness of combining the idea of momentum and Adagrad / RMSProp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
